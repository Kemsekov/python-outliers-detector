{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.metrics as metrics\n",
    "from random import randint\n",
    "from sklearn.calibration import LabelEncoder\n",
    "from common import get_full_data\n",
    "\n",
    "def heart_failure():\n",
    "    df = pd.read_csv(\"dataset/heart_failure_clinical_records_dataset.csv\")\n",
    "    # get dependent and independent features\n",
    "    X=df.iloc[:,:-1]\n",
    "    y=df.iloc[:,-1]\n",
    "    classes = y.unique()\n",
    "    X,y=get_full_data(X,y)\n",
    "    return X,y,[str(c) for c in classes]\n",
    "\n",
    "def encode_categorical_columns(df):\n",
    "    le = LabelEncoder()\n",
    "    # Loop over all columns in the DataFrame\n",
    "    for col in df.columns:\n",
    "        # Check if the column is of type object (string)\n",
    "        if df[col].dtype == 'object':\n",
    "            # Use LabelEncoder to do the numeric transformation\n",
    "            df[col] = le.fit_transform(df[col])\n",
    "    return df\n",
    "\n",
    "def disease():\n",
    "    df = pd.read_csv(\"dataset/disease.csv\")\n",
    "    # get dependent and independent features\n",
    "    symptoms = df.iloc[:,1:]\n",
    "    all_symptoms = set()\n",
    "    for col in symptoms.columns:\n",
    "        unq = symptoms[col].unique()\n",
    "        unq=[str.strip(v) for v in unq if isinstance(v,str)]\n",
    "        all_symptoms.update(unq)\n",
    "    \n",
    "    all_symptoms=list(all_symptoms)\n",
    "    symptoms_embedding = pd.DataFrame(\n",
    "        np.zeros((len(df),len(all_symptoms))),\n",
    "        columns=all_symptoms\n",
    "    )\n",
    "    \n",
    "    uniques = symptoms.apply(lambda x: np.unique(np.array(x.dropna(),dtype=str)),axis=1)\n",
    "    for id,u in enumerate(uniques):\n",
    "        u=[str.strip(v) for v in u if isinstance(v,str)]\n",
    "        symptoms_embedding.iloc[id][u]=1\n",
    "\n",
    "    y = df.iloc[:,0]\n",
    "    classes = y.unique()\n",
    "    \n",
    "    for index,cls in enumerate(classes):\n",
    "        y[y==cls]=index\n",
    "    \n",
    "    return symptoms_embedding, y.astype(int), classes\n",
    "\n",
    "def AIDS():\n",
    "    df = pd.read_csv(\"dataset/AIDS_Classification.csv\")\n",
    "    X=df.iloc[:,:-1]\n",
    "    y=df.iloc[:,-1]\n",
    "    classes = y.unique()\n",
    "    X,y=get_full_data(X,y)\n",
    "    return X,y,[str(c) for c in classes]\n",
    "\n",
    "def seeds():\n",
    "    df = pd.read_csv(\"dataset/seeds.csv\")\n",
    "    y = df.iloc[:,-1]-1\n",
    "    classes = y.apply(lambda x: str(x)).unique()\n",
    "    X = df.iloc[:,:-1]\n",
    "    return X,y, classes\n",
    "\n",
    "def housing():\n",
    "    df = pd.read_csv(\"dataset/housing.csv\")\n",
    "    y=df['SaleCondition']\n",
    "    bad_classes_names=['Alloca','AdjLand','Normal','Partial']\n",
    "    classes = set(y.apply(lambda x: str(x)).unique())-set(bad_classes_names)\n",
    "\n",
    "    X=df.drop(columns=['SaleCondition','SalePrice'])\n",
    "    bad_classes = np.any([y == cl for cl in bad_classes_names],axis=0)\n",
    "    X=encode_categorical_columns(X)\n",
    "\n",
    "    X=np.array(X)\n",
    "    y=np.array(y)\n",
    "\n",
    "    y=y[~bad_classes]\n",
    "    X=X[~bad_classes]\n",
    "\n",
    "\n",
    "    y_encoded = np.zeros_like(y,dtype=np.int32)\n",
    "    for i,cls in enumerate(classes):\n",
    "        y_encoded[y==cls]=i\n",
    "\n",
    "    return X,y_encoded,classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "X,y, y_classes = housing()\n",
    "\n",
    "# for high-dimensional data use `gpu` for device if you have one\n",
    "special_model = XGBClassifier(device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from common import XGB_search_params\n",
    "\n",
    "params = XGB_search_params()\n",
    "state = randint(0,1000)\n",
    "search = RandomizedSearchCV(\n",
    "    special_model,\n",
    "    params,\n",
    "    n_iter=200,\n",
    "    cv=5,\n",
    "    random_state=state,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# search.fit(X,y)\n",
    "# special_model=search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do repeated stratified k-fold cross-validation with classification report\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from common import cross_val_classification_report\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=50)\n",
    "report = cross_val_classification_report(\n",
    "    model=special_model,\n",
    "    X=np.array(X),\n",
    "    y=y,\n",
    "    cv=cv,\n",
    "    target_names=y_classes\n",
    ")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New method\n",
    "from common import find_outliers\n",
    "\n",
    "X_numpy = np.array(X)\n",
    "y_numpy = np.array(y)\n",
    "\n",
    "outliers_mask, score = find_outliers(\n",
    "    X_numpy,\n",
    "    y_numpy,\n",
    "    special_model,\n",
    "    outliers_to_remove=0.3,\n",
    "    iterations=5,\n",
    "    gamma=0.9,\n",
    "    evaluate_loss=metrics.mean_squared_error,\n",
    "    cv=5,\n",
    "    repeats=3,\n",
    "    class_weight_scale_power=0.5,\n",
    "    plot=True\n",
    ")\n",
    "\n",
    "X_clean = X_numpy[~outliers_mask]\n",
    "y_clean = y_numpy[~outliers_mask]\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=50)\n",
    "report = cross_val_classification_report(\n",
    "    model=special_model,\n",
    "    X=X_clean,\n",
    "    y=y_clean,\n",
    "    cv=cv,\n",
    "    target_names=y_classes\n",
    ")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import generate_colors_for_classification\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from kernel_pca_search import KernelPCASearchCV, kernel_pca_scorer\n",
    "from render import *\n",
    "\n",
    "scaler = RobustScaler()\n",
    "\n",
    "X_numpy[np.isnan(X_numpy)]=-1\n",
    "X_ = scaler.fit_transform(X_numpy)\n",
    "\n",
    "transform = KernelPCASearchCV(n_components=3,n_iter=-1,kernel='rbf')\n",
    "x_transform = transform.fit_transform(X_)\n",
    "\n",
    "colors = generate_colors_for_classification(y,seed=100)\n",
    "data = np.concatenate([x_transform,colors],axis=1)\n",
    "plot_3d_rgb(data,\"Original data plot\",[\"d1\",\"d2\",\"d3\"],6,None)\n",
    "\n",
    "colors[outliers_mask]=(0,0,0)\n",
    "data = np.concatenate([x_transform,colors],axis=1)\n",
    "plot_3d_rgb(data,\"Cleaned data plot\",[\"d1\",\"d2\",\"d3\"],6,None)\n",
    "\n",
    "print(kernel_pca_scorer(transform,X_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare performance with some control model `SVC` with `rbf` kernel and standard scaler\n",
    "\n",
    "Find optimal hyperparameters using `RandomSearchCV`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
